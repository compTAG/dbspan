{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8e40eef",
   "metadata": {},
   "source": [
    "# Experiments for \"DBSpan: Density-Based Spanner for Clustering Complex Data, With an Application to Persistence Diagrams\"\n",
    "\n",
    "To get started, we will do a little bit of path hackery to import the library.  There are better ways to do this, but for now, this is okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d093c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "\n",
    "libpath = os.path.abspath('..')\n",
    "sys.path.insert(0, libpath)\n",
    "\n",
    "import dbspan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82486f89",
   "metadata": {},
   "source": [
    "We will do a bunch of comparisions of DBScan and DBSpan, so it will help if we have some code for making the algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39afd857",
   "metadata": {},
   "outputs": [],
   "source": [
    "dgm_metric = dbspan.topology.DiagramMetric()\n",
    "\n",
    "def dgm1_metric(dgm1, dgm2):\n",
    "    return dgm_metric.bottleneck(dgm1[1], dgm2[1])\n",
    "\n",
    "def make_algos(eps, min_samples, delta):\n",
    "    algo_dbscan = dbspan.cluster.DBSCAN(metric=dgm1_metric, eps=eps, min_samples=min_samples)\n",
    "    algo_dbspan = dbspan.cluster.DBSpan(metric=dgm1_metric, eps=eps, min_samples=min_samples, delta=delta)\n",
    "\n",
    "    return algo_dbscan, algo_dbspan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5c0c97",
   "metadata": {},
   "source": [
    "## Experiment 1: Point clouds of things in 4D\n",
    "\n",
    "First, we will create some tools for creating dgms of things in 4D.  Each function will produce a diagram from a rips filtration of a \"noisy\" sphere, torus, or swiss roll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cc574c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_factory = dbspan.topology.PointSetDataFactory(seed=72330)\n",
    "dgm_factory = dbspan.topology.DiagramFactory()\n",
    "\n",
    "rng = random.Random(43081)\n",
    "\n",
    "def make_sphere_dgm():\n",
    "    points = data_factory.make_sphere(dim=4, num_points=100, noise=.1)\n",
    "    return dgm_factory.make_from_point_set(points)\n",
    "\n",
    "def make_torus_dgm():\n",
    "    points = data_factory.make_torus(dim=4, num_points=100, noise=.1)\n",
    "    return dgm_factory.make_from_point_set(points)\n",
    "\n",
    "def make_swiss_roll_dgm():\n",
    "    points = data_factory.make_swiss_roll(dim=4, num_points=100, noise=.1)\n",
    "    return dgm_factory.make_from_point_set(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f567ad8c",
   "metadata": {},
   "source": [
    "Now that we have some functions for creating data, let's create our data set that we will cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b4ab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dgms1 = 30\n",
    "dgms1 = [make_sphere_dgm() for _ in range(num_dgms1)] \\\n",
    "    + [make_torus_dgm() for _ in range(num_dgms1)] \\\n",
    "    + [make_swiss_roll_dgm() for _ in range(math.floor(num_dgms1/2) - 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52efd5c7",
   "metadata": {},
   "source": [
    "And we are off!  Let's do some experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d931b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "def make_cache(true_labels, dbscan_time):\n",
    "    return {\n",
    "        'true_labels': true_labels,\n",
    "        'dbscan_time': dbscan_time,\n",
    "    }  \n",
    "    \n",
    "def run_experiment1(df, data, delta, eps, min_samples, cache=None):\n",
    "    \n",
    "    def add_row(df, delta, num_edges, max_edges, rand_index, dbscan_time, dbspan_time):\n",
    "        data = [delta, rand_index, num_edges, max_edges, num_edges/max_edges, dbscan_time, dbspan_time, dbscan_time/dbspan_time]\n",
    "        cols=['$\\delta$', 'Rand index', 'Num Edges', 'maxEdges', '\\% Possible Edges', 'T_DBSCAN', 'DBSpan time (sec)', 'Speedup']\n",
    "        line = pd.DataFrame([data,], columns=cols)\n",
    "        return pd.concat([df, line])\n",
    "   \n",
    "    # create the algos\n",
    "    algo_dbscan, algo_dbspan = make_algos(eps=eps, min_samples=min_samples, delta=delta)\n",
    "\n",
    "    # run the algos\n",
    "    t0 = time.perf_counter()\n",
    "    true_labels = cache['true_labels'] if cache else algo_dbscan.fit(data)\n",
    "    t1 = time.perf_counter()\n",
    "    dbspan_labels, dbg_data= algo_dbspan.fit(data, dbg=True)\n",
    "    t2 = time.perf_counter()\n",
    "    \n",
    "    # pull out the dbg data for the analysis\n",
    "    spanner = dbg_data['neighborhood'].spanner\n",
    "    \n",
    "    # prepare data for row\n",
    "    rand_index = metrics.adjusted_rand_score(true_labels, dbspan_labels)\n",
    "    dbscan_time = cache['dbscan_time'] if cache else t1 - t0\n",
    "    dbspan_time = t2 - t1\n",
    "    num_edges = spanner.number_of_edges()\n",
    "    n = spanner.number_of_nodes()\n",
    "    max_edges = n * (n-1) / 2\n",
    "    \n",
    "    # return row\n",
    "    return add_row(df, delta, num_edges, max_edges, rand_index, dbscan_time, dbspan_time), \\\n",
    "        make_cache(true_labels, dbscan_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6668efe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results1 = None\n",
    "cache1 = None\n",
    "eps1 = .3\n",
    "min_samples1 = 15\n",
    "for delta in [.1, 1, 10, 50, 100, 500, 1000]:\n",
    "    results1, cache1 = run_experiment1(results1, dgms1, delta, eps1, min_samples1, cache=cache1)\n",
    "    print(results1)\n",
    "\n",
    "print(results1.style.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241f9da1",
   "metadata": {},
   "source": [
    "# Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b96dfc",
   "metadata": {},
   "source": [
    "First, we will load in the data.  Since it is pretty small when gzipped, we keep it in the repo.  Let's unzip it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d4ad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data\n",
    "!curl -J -L https://osf.io/4nwe9/download | tar xz --directory data\n",
    "!echo \"done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aecee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "exp2_data_dir = './data/experiment2/'\n",
    "exp2_files = os.listdir(exp2_data_dir)\n",
    "\n",
    "def read_file(name):\n",
    "    file_name = os.path.join(exp2_data_dir, name)\n",
    "    return [None, np.genfromtxt(file_name, delimiter=',')]\n",
    "\n",
    "dgms2 = [ read_file(name) for name in exp2_files ]\n",
    "print(\"Read in {} dgms\".format(len(dgms2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b78d6e",
   "metadata": {},
   "source": [
    "Next, let's look at the distribution of the number of points in the diagrams.  Personally, I don't know much about this data set, but MSU local expert (and super well guy that gave me these diagrams) Jordan Schupbach said that number of diagram points could be a useful to look at.  So let's look at the distribution of of the number of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed32fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "npts = np.array([dgm[1].shape[0] for dgm in dgms2])\n",
    "plt.hist(npts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfb8ab5",
   "metadata": {},
   "source": [
    "So, it looks like we have a bunch of smallish dgms and a few very large diagrams.  To get a good distribution of diagram sizes, we will create three levels each containing the same number of diagrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7f3d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "dgms_by_level = [\n",
    "    np.argwhere(npts < 150),\n",
    "    np.argwhere((150 <= npts) & (npts < 200)),\n",
    "    np.argwhere(200 <= npts),\n",
    "]\n",
    "\n",
    "def make_data2(dgms_per_level):\n",
    "    rng = np.random.default_rng(seed=0)\n",
    "    return [\n",
    "        dgms2[idx]\n",
    "        for level in dgms_by_level\n",
    "        for idx in rng.choice(level.transpose()[0], size=dgms_per_level, replace=False)\n",
    "    ]\n",
    "\n",
    "small_data2 = make_data2(dgms_per_level=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ed5816",
   "metadata": {},
   "source": [
    "Next, let's look at the pairwise distance matrix of a very small dataset.  (Note that the code below takes quite a while to run and stare at to pick some constants.  Uncomment to check out the distances)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe2aa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(small_data2)):\n",
    "#     for j in range(i+1, len(small_data2)):\n",
    "#         print(i, j, dgm1_metric(small_data2[i], small_data2[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e22801",
   "metadata": {},
   "source": [
    "So, it looks like we have a bunch of close diagrams all with distance less than 10 and then some outliers.  So, let's cluster our points using and epsilon of 10 and see how it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ab778f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps2 = 10\n",
    "min_samples2 = 5\n",
    "\n",
    "algo_dbscan = dbspan.cluster.DBSCAN(metric=dgm1_metric, eps=eps2, min_samples=min_samples2)\n",
    "\n",
    "# run the algos\n",
    "t0 = time.perf_counter()\n",
    "true_labels = algo_dbscan.fit(small_data2)\n",
    "t1 = time.perf_counter()\n",
    "dbscan_time = t1-t0\n",
    "cache2 = make_cache(true_labels, dbscan_time)\n",
    "print(cache2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461019c6",
   "metadata": {},
   "source": [
    "So, as we expected, we have a one cluster of diagrams and some noise.  Now, let's run the same experiments that we ran before to pick a good value for delta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73880b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = None\n",
    "for delta in [.1, 1, 2, 3, 4, 5, 6]:\n",
    "    results2, cache2 = run_experiment1(results2, small_data2, delta, eps2, min_samples2, cache=cache2)\n",
    "    print(results2)\n",
    "print(results2.style.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8ee83a",
   "metadata": {},
   "source": [
    "Excellent!!  So what did we learn, well, we can get a pretty good speed up and maintain pretty good accuracy if we pick delta just before the rand index dropps off.  Now let's see how large of a data set we can process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af78462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta2 = 1\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    level=logging.INFO,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def run_experiment2(df, num_per_level, delta, eps, min_samples):\n",
    "  \n",
    "    def add_row(df, num_dgms, num_edges, max_edges, rand_index, dbscan_time, dbspan_time):\n",
    "        data = [num_dgms, rand_index, num_edges, max_edges, num_edges/max_edges, dbscan_time, dbspan_time, dbscan_time/dbspan_time]\n",
    "        cols=['Num Dgms', 'Rand index', 'Num Edges', 'maxEdges', '\\% Possible Edges', 'T_DBSCAN', 'DBSpan time (sec)', 'Speedup']\n",
    "        line = pd.DataFrame([data,], columns=cols)\n",
    "        return pd.concat([df, line])\n",
    "  \n",
    "    d2 = make_data2(dgms_per_level=num_per_level)\n",
    "    \n",
    "    algo_dbscan, algo_dbspan = make_algos(eps=eps, min_samples=min_samples, delta=delta)\n",
    "\n",
    "    # run the algos\n",
    "    t0 = time.perf_counter()\n",
    "    dbspan_labels, dbg_data= algo_dbspan.fit(d2, dbg=True)\n",
    "    dbspan_time = time.perf_counter() - t0\n",
    "\n",
    "    # prep dbspan data\n",
    "    spanner = dbg_data['neighborhood'].spanner    \n",
    "    num_edges = spanner.number_of_edges()\n",
    "    n = spanner.number_of_nodes()\n",
    "    max_edges = n * (n-1) / 2\n",
    "    \n",
    "    logging.info((n, dbspan_time, num_edges, max_edges))\n",
    "    \n",
    "    # run dbscan\n",
    "    t1 = time.perf_counter()\n",
    "    true_labels = algo_dbscan.fit(d2)\n",
    "    dbscan_time = time.perf_counter() - t1\n",
    "    \n",
    "    # prepare data for row\n",
    "    rand_index = metrics.adjusted_rand_score(true_labels, dbspan_labels)   \n",
    "\n",
    "    # return row\n",
    "    return add_row(df, n, num_edges, max_edges, rand_index, dbscan_time, dbspan_time)\n",
    "    \n",
    "results2b = None\n",
    "eps2 = 10\n",
    "min_samples2 = 5\n",
    "for i in [10, 15, 20, 25, 30, 35, 40]:\n",
    "    results2b = run_experiment2(results2b, i, delta2, eps2, min_samples2)\n",
    "    print(results2b)\n",
    "\n",
    "print(results2b.style.to_latex())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
